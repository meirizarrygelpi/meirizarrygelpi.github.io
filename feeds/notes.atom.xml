<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>M.E. Irizarry-Gelpí</title><link href="http://meirizarrygelpi.github.io/" rel="alternate"></link><link href="http://meirizarrygelpi.github.io/feeds/notes.atom.xml" rel="self"></link><id>http://meirizarrygelpi.github.io/</id><updated>2015-06-13T00:00:00-04:00</updated><entry><title>Introduction to Big Data and Data Science</title><link href="http://meirizarrygelpi.github.io/posts/2015/Jun/intro-big-data-data-science/" rel="alternate"></link><updated>2015-06-13T00:00:00-04:00</updated><author><name>M.E. Irizarry-Gelpí</name></author><id>tag:meirizarrygelpi.github.io,2015-06-13:posts/2015/Jun/intro-big-data-data-science/</id><summary type="html">&lt;p&gt;&lt;em&gt;These are my notes from lecture 1 of the MOOC CS100.1x Introduction to Big Data with Apache Spark by BerkeleyX.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The outline of this lecture is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Brief History of Data Analysis&lt;/li&gt;
&lt;li&gt;Big Data and Data Science: Why All the Excitement?&lt;/li&gt;
&lt;li&gt;Where Big Data Comes From?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Full disclosure: I have a technical background, but I do not have much experience with statistics or working with data. Hopefully, that will change soon. In the mean time, I am providing my own interpretation of the material provided in this lecture.&lt;/p&gt;
&lt;p&gt;It is always a good idea to look back and provide a brief history of how things have change leading up to the current state. In the mid 1930s R.A. Fisher proposed "The Design of Experiments", along with some statistical tests. I suppose this was on the topic of designing experiments that would accurately lead to proving or disproving hypothesis and thus gaining knowledge. Fisher is also credited with the statement "correlation does not imply causation". This is a statement about how one interprets an outcome from a statistical experiment.&lt;/p&gt;
&lt;p&gt;Towards the end of the 1930s W.E. Demming propose the idea of quality control using statistical sampling. I guess this is an application of statistics to improving the quality of products made by a business, so it serves as an example of how statistics can make a company better.&lt;/p&gt;
&lt;p&gt;Later in the 1950s P. Luhn in "A Business Intelligence System" propose using indexing and information retrieval methods with text and data for business intelligence. According to Wikipedia:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Business Intelligence (BI) is the set of techniques and tools for the transformation of raw data into meaningful and useful information for business analysis purposes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So apparently Luhn was one of the first to suggest a way of interacting with the business data that is similar to the way it is done currently.&lt;/p&gt;
&lt;p&gt;In 1977 J.W. Tukey wrote the book "Exploratory Data Analysis" which lead later to the development of the programming languages S, S-PLUS and also R. According to Wikipedia:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Exploratory Data Analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So when you do EDA, you are basically getting to know the data.&lt;/p&gt;
&lt;p&gt;As the years passed, technology and businesses change greatly. Towards the end of the 1980s H. Dresner proposed a modern approach to BI.&lt;/p&gt;
&lt;p&gt;The book on machine learning by T. Mitchell from 1997 is mentioned. I guess in 2015 it is still a bestseller. Perhaps this was one of the first signs that machine learning could be useful and important for businesses.&lt;/p&gt;
&lt;p&gt;The search engine Google started in 1996 by two PhD candidates in Stanford University. Google search was immensely useful in making the Internet useful for everyone. More abstractly, it helped organized the content and information on the Internet. It can also be viewed as an example of a big data problem.&lt;/p&gt;
&lt;p&gt;In 2007 Microsoft released the eBook &lt;a href="http://research.microsoft.com/en-us/collaboration/fourthparadigm/"&gt;The Fourth Paradigm&lt;/a&gt; on data-driven science. The idea is that besides the traditional "small science" and "mid science" projects, there are "big science" projects like the Large Hadron Collider. New tools and new methods are needed in order to make new discoveries. Presumably these remarks about data-driven science can be adapted to data-driven businesses and industry.&lt;/p&gt;
&lt;p&gt;Then in 2009 P. Norvig and others published "The Unreasonable Effectiveness of Data" where they presented the idea that "multiple small models and lots of data is much more effective than building very complex models".&lt;/p&gt;
&lt;p&gt;So extracting information from data is a sure way to obtain evidence and knowledge. Well, not always. There are some steps to be taken in order for the analysis of data to lead to correct conclusions. There is the example of a study by A. Keys where he found a correlation between the fat calories consumed and deaths by degenerative heart disease. But there was a lot of controversy about this study because, among other things, he only studied a subset of all the countries involve (a selection bias?) and also failed to consider other factors. Always remember: correlation does not imply causation.&lt;/p&gt;
&lt;p&gt;Like most task, there are common ways to do the task wrong, but there are also ways of doing the task right. There are ways of extracting correct information from data. Many companies have been acquiring lots of data for many years. With the advent of powerful analytical tools, it is hoped that all of these data will lead to better businesses.&lt;/p&gt;
&lt;p&gt;One of the things that can be done with lots of data is nowcasting. Traditionally, data is used for forecasting. That is, they use data to make a model that predicts the future. With nowcasting you collect a lot of data and you build a model to explain what is happening in now. An example of this is Google Flu Trends, where data from Google searches was used to determined when influenza outbreaks were happening. Traditionally, outbreaks are declared by the CDC after receiving data from state health departments, who receive data from county health departments, who receive data from local town departments, who receive data from doctors and hospitals who treat sick people. In the traditional process it takes many weeks for the data to be communicated along the hierarchy, so there is a long period of time where, if there is an outbreak, nothing is being about it. Google developed a model based on analyzing data from searches during known outbreaks and extracting common search terms associated to people with the flu. Eventually, in 2010 they were able to predict an outbreak two weeks before the CDC. For a while the model was very accurate with CDC data, about 97% agreement. But during a time period the model disagree with CDC data by 200%, i.e. it was predicting twice as many flu cases as the CDC was finding. The reason was that people were reading and searching for flu on the Internet and skewing the results for the model. After taking into account these factors, the model became again accurate. Later, similar models were developed to predict other outbreaks, like Ebola. The take-away from this example is that just because a given model works well one time it does not necessarily mean that it will always work well. A healthy dose of skepticism seems to be a good thing to have when working with data. Also, being willing to always allow room for improvement.&lt;/p&gt;
&lt;p&gt;Google Flu Trends is an example where search trends was used to make accurate predictions (in this case, about flu outbreaks). This does not mean that every analysis of search trends will lead to correct conclusions: correlation does not imply causation. There is the example of some researchers from Princeton University that used search trends for "MySpace" to predict the demise of another social network, Facebook. This essentially assumed that the correlation in the decrease of searches for MySpace was the causation of MySpace's demise. Facebook &lt;a href="https://www.facebook.com/notes/mike-develin/debunking-princeton/10151947421191849"&gt;responded&lt;/a&gt; in turn by providing a bunch of examples of "causal correlations".&lt;/p&gt;
&lt;p&gt;Increasingly more and more things are done over the Internet. Whether is via a computer, a smartphone, or a tablet, many aspects of a person's interaction with other people or services via the Internet are recorded as data. Not all of this data is analyzed. I will refer to this data as online activity data. Another source of data is the user who produces content. This data is referred to as user generated content. A single user might not generate much data, but considering all users of a single service leads to very large data sets. Yet another source of big data is any "big science" project like the LHC.&lt;/p&gt;
&lt;p&gt;Graphs are convenient ways to encode connections between objects. In a social network like Facebook, graphs can be very large.&lt;/p&gt;
&lt;p&gt;Log files are another source of big data. These are files that are generated by an application that contain a record of the activity performed by that application. For example, some web servers logs contain a record of every click. Accounting for many servers leads to very large data sets.&lt;/p&gt;
&lt;p&gt;An emerging source of big data is the so-called Internet of Things, where objects are equipped with sensors that gather all sorts of data.&lt;/p&gt;</summary><category term="big data"></category><category term="mooc"></category></entry><entry><title>Algorithm Theory Problems (Week 1)</title><link href="http://meirizarrygelpi.github.io/posts/2014/May/algorithms-theory-problems-week-1/" rel="alternate"></link><updated>2014-05-04T00:00:00-04:00</updated><author><name>M.E. Irizarry-Gelpí</name></author><id>tag:meirizarrygelpi.github.io,2014-05-04:posts/2014/May/algorithms-theory-problems-week-1/</id><summary type="html">&lt;p&gt;&lt;em&gt;These problems were taken from Tim Roughgarden's course on algorithms.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here are some theory problems that I should consider solving in the near future.&lt;/p&gt;
&lt;h1&gt;Problem 1&lt;/h1&gt;
&lt;p&gt;You are given as input an unsorted array of $n$ distinct numbers, where $n$ is a power of $2$. Provide an algorithm that identifies the second largest number in the array, and that uses at most $n + \lg{(n)} - 2$ comparisons.&lt;/p&gt;
&lt;h1&gt;Problem 2&lt;/h1&gt;
&lt;p&gt;You are given a unimodal array of $n$ distinct elements, meaning that its entries are in increasing order up until its maximum element, after which its elements are in decreasing order. Provide an algorithm to compute the maximum element that runs in $O(\log(n))$ time.&lt;/p&gt;
&lt;h1&gt;Problem 3&lt;/h1&gt;
&lt;p&gt;You are given a sorted (from smallest to largest) array $A$ of $n$ distinct integers which can be positive, negative, or zero. You want to decide whether or not there is an index $i$ such that $A_{i} = i$. Design the fastest algorithm that you can for solving this problem.&lt;/p&gt;
&lt;h1&gt;Problem 4&lt;/h1&gt;
&lt;p&gt;Give the best upper bound that you can on the solution to the following recurrence:$$T(1) = 1, \qquad T(n) \leq T([\sqrt{n}]) + 1, \qquad n &amp;gt; 1$$(Here $[x]$ denotes the "floor" function, which rounds down to the nearest integer.)&lt;/p&gt;
&lt;h1&gt;Problem 5&lt;/h1&gt;
&lt;p&gt;You are given an $n \times n$ grid of distinct numbers. A number is a local minimum if it is smaller than all of its neighbors. (A neighbor of a number is one immediately above, below, to the left, or the right. Most numbers have four neighbors; numbers on the side have three; the four corners have two.) Use the divide-and-conquer algorithm design paradigm to compute a local minimum with only $O(n)$ comparisons between pairs of numbers. (&lt;strong&gt;Note:&lt;/strong&gt; since there are $n^{2}$ numbers in the input, you cannot afford to look at all of them. &lt;strong&gt;Hint:&lt;/strong&gt; think about what types of recurrences would give you the desired upper bound.)&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#3e3f3a ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="algorithms"></category></entry><entry><title>Algorithms (Week 1)</title><link href="http://meirizarrygelpi.github.io/posts/2014/May/algorithms-week-1/" rel="alternate"></link><updated>2014-05-04T00:00:00-04:00</updated><author><name>M.E. Irizarry-Gelpí</name></author><id>tag:meirizarrygelpi.github.io,2014-05-04:posts/2014/May/algorithms-week-1/</id><summary type="html">&lt;p&gt;As part of my journey towards data science I am taking a MOOC on algorithms by Tim Roughgarden from Stanford University. I took my first MOOC some years ago from MIT and enjoyed it quite a lot. Algorithms are things I know very little about and I hope this is no longer true by the end of the course. As part of my study I will keep some lecture notes with important points from each of the six weeks. Currently, I am planning on solving the programming questions with Python (using this as an opportunity to write some code in Python 3.x).&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;algorithm&lt;/strong&gt; is "a set of well-defined rules, a recipe in effect, for solving some computational problem".&lt;/p&gt;
&lt;p&gt;Define a computational problem, with an &lt;em&gt;input&lt;/em&gt; and an &lt;em&gt;output&lt;/em&gt;, then provide an algorithm that transforms the input into the output.&lt;/p&gt;
&lt;p&gt;The algorithm's designer mantra:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Perhaps the most important principle for the good algorithm designer is to refuse to be content." - Aho, Hopcroft, and Ullman, &lt;em&gt;The Design and Analysis of Computer Algorithms&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words: &lt;strong&gt;&lt;em&gt;Can we do better?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Merge Sort&lt;/h3&gt;
&lt;p&gt;Merge sort is an old algorithm. A good example of the divide-and-conquer paradigm. One breaks a problem into smaller sub-problems that are solved recursively. Merge sort achives sorting with a number of operations that is less than quadratic in size.&lt;/p&gt;
&lt;p&gt;The sorting problem: as input we are given an array of $N$ numbers, which are not sorted, and we are asked to provide the $N$ numbers in sorted order (say in increasing order).&lt;/p&gt;
&lt;p&gt;I wrote a short function that takes as input a non-negative integer &lt;code&gt;n&lt;/code&gt;, and gives as output an unsorted list containing the &lt;code&gt;n + 1&lt;/code&gt; integers between &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt;. Here it is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;choice&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;unsorted_list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;l1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;l2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;l1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;l2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With this function I can create list of integers that need to be sorted. Next, we implement Merge Sort. My implementation involves two parts. The first part is a function that takes as input two sorted lists of integers &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;, and gives as output a third sorted list &lt;code&gt;c&lt;/code&gt; made by merging &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;. Here is the code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mel_merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;i_a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;i_b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_a&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_b&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_a&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_b&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_a&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;i_a&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_b&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_a&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
            &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_b&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;i_b&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_a&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_b&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;i_a&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;i_b&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_b&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_b&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_b&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i_a&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i_a&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The second part is a recursive function that takes an unsorted list &lt;code&gt;l&lt;/code&gt;, splits it into two smaller lists, then recursively calls itself on those smaller lists producing sorted lists &lt;code&gt;l1&lt;/code&gt; and &lt;code&gt;l2&lt;/code&gt;, and finally calls &lt;code&gt;mel_merge&lt;/code&gt; to merge these two into a sorted list:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;mel_merge_sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;len_l&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;len_l&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;len_l&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;l&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;len_l&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
        &lt;span class="n"&gt;l1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mel_merge_sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;l2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mel_merge_sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;:])&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;mel_merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;l2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Analysis of Algorithms&lt;/h3&gt;
&lt;p&gt;One can adopt three guiding principles for how to reason about algorithms and also how to define a fast algorithm.&lt;/p&gt;
&lt;p&gt;The first guiding principle is &lt;strong&gt;worst-case analysis&lt;/strong&gt;. This principle will help us find results that hold quite generally with no assumptions about the specific details of the input. A related, but different principle is &lt;strong&gt;average-case analysis&lt;/strong&gt;, where one makes some assumptions about the frequencies of certain kinds of inputs and develops algorithms optimized for those particular cases. However, this requires some domain knowledge, whereas the worst-case analysis holds in general.&lt;/p&gt;
&lt;p&gt;The second guiding principle consist of not worrying to much about constant factors, or lowest order terms. This means that we are really interested in looking at the leading (dominant) behavior.&lt;/p&gt;
&lt;p&gt;The third guiding principle consist of using asymptotic analysis. This means that we will focus on large input sizes. In some sense, the interesting problems are the ones with large inputs.&lt;/p&gt;
&lt;p&gt;With these three principles we can define a &lt;strong&gt;fast algorithm&lt;/strong&gt; as an algorithm where the wors-case running time grows slowly with the input size. For most of the problems we will encounter, the "holy grail" is to find an algorithm that solves the problem in linear time (or close to linear time).&lt;/p&gt;
&lt;h2&gt;Asymptotic Analysis&lt;/h2&gt;
&lt;p&gt;Asymptotic analysis is the language used to discuss the high-level performance of an algorithm. It is coarse enough to suppress specific details like architecture/language/compiler-dependent details. It is also sharp enough to make useful comparisons between different algorithms, especially on large inputs.&lt;/p&gt;
&lt;p&gt;The high-level idea is to supress constant factors (which are system-specific) and lower-order terms (which are irrelevant for large inputs). A somewhat careful analysis of Merge Sort yields a running time of the form$$T(n) = 6n \lg(n) + 6n$$Using asymptotic analysis we just provide $n \lg(n)$. That is, we ignore the constant factor $6$ and the lower-order term $6n$. In this sense we say that the running time of Merge Sort is $O(n \lg(n))$.&lt;/p&gt;
&lt;h3&gt;Big-O&lt;/h3&gt;
&lt;p&gt;In general, we only consider functions defined for positive integers (i.e. sizes of inputs). One such function is the worst-case running time of an algorithm as a function of the input size $n$, which we denote by $T(n)$. If $T(n) = O(f(n))$, this means that eventually (i.e. for all sufficiently large $n$), $T(n)$ is bounded from &lt;em&gt;above&lt;/em&gt; by $f(n)$. A more formal definition is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Big-O Notation:&lt;/strong&gt; $T(n) = O(f(n))$ if and only if there exist positive constants $c$ and $n_{0}$ such that$$T(n) \leq c f(n)$$for all $n \geq n_{0}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Big-Omega and Big-Theta&lt;/h3&gt;
&lt;p&gt;Similar to big-O, we use $T(n) = \Omega(g(n))$ to mean that eventually $T(n)$ is bounded from &lt;em&gt;below&lt;/em&gt; by $g(n)$. A more formal definition is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Big-Omega Notation:&lt;/strong&gt; $T(n) = \Omega(g(n))$ if and only if there exist positive constants $c$ and $n_{0}$ such that$$T(n) \geq c g(n)$$for all $n \geq n_{0}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Finally, we can use $T(n) = \Theta(h(n))$ to mean that both $T(n) = O(h(n))$ and $T(n) = \Omega(h(n))$. That is,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Big-Theta Notation:&lt;/strong&gt; $T(n) = \Theta(h(n))$ if there exist positive constants $c_{1}$, $c_{2}$ and $n_{0}$ such that $$c_{1} h(n) \leq T(n) \leq c_{2} h(n)$$for all $n \geq n_{0}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Divide and Conquer Algorithms&lt;/h2&gt;
&lt;p&gt;The divide and conquer paradigm for solving problems consist of two main steps. One first divides the main problem into smaller subproblems, that are solvable (the conquering). After combining the solutions of the smaller subproblems, the starting problem can be solved.&lt;/p&gt;
&lt;h3&gt;Counting Inversions&lt;/h3&gt;
&lt;p&gt;Given an array $A$ of $n$ distinct integers, an &lt;strong&gt;inversion&lt;/strong&gt; is a pair of array indices $i$ and $j$ such that $i &amp;lt; j$ and $A_{i} &amp;gt; A_{j}$.&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: '#3e3f3a ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="algorithms"></category></entry></feed>