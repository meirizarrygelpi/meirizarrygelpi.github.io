<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  Deep Learning Fundamentals with Keras 2 | M.E. Irizarry-Gelpí
</title>
  <link rel="canonical" href="https://meirizarrygelpi.github.io/posts/data/deep-learning-fundamentals-with-keras-2/index.html">


  <link rel="stylesheet" href="https://meirizarrygelpi.github.io/theme/css/litera.min.css">
  <link rel="stylesheet" href="https://meirizarrygelpi.github.io/theme/css/fontawesome.min.css">
  <link rel="stylesheet" href="https://meirizarrygelpi.github.io/theme/css/pygments/dracula.min.css">
  <link rel="stylesheet" href="https://meirizarrygelpi.github.io/theme/css/theme.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="https://meirizarrygelpi.github.io/feeds/all.atom.xml">
  <link rel="alternate" type="application/atom+xml" title="Atom Feed"
        href="https://meirizarrygelpi.github.io/feeds/main.xml">  <link rel="alternate" type="application/atom+xml" title="Categories Atom Feed"
        href="https://meirizarrygelpi.github.io/feeds/data.atom.xml">  
  <meta name="description" content="The second module of the Deep Learning Fundamentals with Keras course.">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
    <div class="col-sm-4">
      <a href="https://meirizarrygelpi.github.io/">
        <img class="img-fluid rounded" src=https://meirizarrygelpi.github.io/images/melvin-prisma.jpg alt="M.E. Irizarry-Gelpí">
      </a>
    </div>
  <div class="col-sm-8">
    <h1 class="title"><a href="https://meirizarrygelpi.github.io/">M.E. Irizarry-Gelpí</a></h1>
      <p class="text-muted">Physics impostor. Mathematics interloper. Husband. Father.</p>
      <ul class="list-inline">
          <li class="list-inline-item"><a href="https://meirizarrygelpi.github.io/pages/projects/" target="_blank">projects</a></li>
          <li class="list-inline-item"><a href="https://meirizarrygelpi.github.io/pages/about/" target="_blank">about</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fab fa-twitter" href="https://twitter.com/melvineloy" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-github" href="https://github.com/meirizarrygelpi" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-bitbucket" href="https://bitbucket.org/meirizarrygelpi" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fas fa-rss" href="https://meirizarrygelpi.github.io/feeds/main.xml" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>  Deep Learning Fundamentals with Keras 2
</h1>
      <hr>
  <article class="article">
    <header>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="1898-09-28T00:00:00-04:56">
          <i class="fas fa-clock"></i>
          Wed 28 September 1898
        </li>
        <li class="list-inline-item">
          <i class="fas fa-folder-open"></i>
          <a href="https://meirizarrygelpi.github.io/category/data.html">Data</a>
        </li>
          <li class="list-inline-item">
            <i class="fas fa-tag"></i>
              <a href="https://meirizarrygelpi.github.io/tag/keras.html">#keras</a>          </li>
      </ul>
    </header>
    <div class="content">
      <p>Say that you have some data and wish to fit a function of the form
</p>
<div class="math">\begin{equation*}
    Z = w X
\end{equation*}</div>
<p>
You need to find the value of <span class="math">\(w\)</span> that best fits the data. One way to do this is to work with a <strong>loss function</strong> that is convex. For example,
</p>
<div class="math">\begin{equation*}
    J(w) = \frac{1}{2 N} \sum_{j = 1}^{N} \left( z_{j} - w x_{j} \right)^{2}
\end{equation*}</div>
<p>
The best fit corresponds to the value of <span class="math">\(w\)</span> that minimizes the loss function <span class="math">\(J(w)\)</span>. Note that the loss function is a function in weight (and bias) space.</p>
<p>In general, the weight values for the best fit can be determined with the <strong>gradient descent algorithm</strong>. With this approach, you take steps that are proportional to the gradient of the cost function at the current point. You start with random weights <span class="math">\(w_{0}\)</span> and compute the value of the gradient of the loss function at that initial weight value. This gradient is
</p>
<div class="math">\begin{equation*}
    \frac{\partial J}{\partial w}
\end{equation*}</div>
<p>
The gradient determines the direction of the step in weight space. The size of the step in weight space is controlled by a parameter <span class="math">\(\eta\)</span> called the <strong>learning rate</strong>. The bigger the learning rate is, the larger the step in weight space will be. The <strong>new</strong> value of the weight is
</p>
<div class="math">\begin{equation*}
    w_{1} = w_{0} - \eta \frac{\partial J}{\partial w}
\end{equation*}</div>
<p>
This is the first iteration of the gradient descent algorithm.</p>
<p>The value of the learning rate needs to be chosen with care. If it is too large, you can end with large steps. If it is too small, you can end with a long-running algorithm.</p>
<p>In order for a neural network to provide good results, it needs to optimize the weights and biases it uses. One way to do this is with the <strong>backpropagation algorithm</strong>. During training, you have input data <span class="math">\(x\)</span> and also ground truth <span class="math">\(T\)</span>. For simplicity assume a neural network with one input, and two layers. In the first layer,
</p>
<div class="math">\begin{equation*}
    x_{1} \longrightarrow z_{1} = w_{1} x_{1} + b_{1} \longrightarrow a_{1} = f(z_{1})
\end{equation*}</div>
<p>
In the second layer,
</p>
<div class="math">\begin{equation*}
    a_{1} \longrightarrow z_{2} = w_{2} a_{1} + b_{2} \longrightarrow a_{2} = f(z_{2})
\end{equation*}</div>
<p>
Thus, <span class="math">\(a_{1}\)</span> depends on the weight <span class="math">\(w_{1}\)</span> and bias <span class="math">\(b_{1}\)</span>, but the output <span class="math">\(a_{2}\)</span> depends on the weight <span class="math">\(w_{2}\)</span> and bias <span class="math">\(b_{2}\)</span> as well as the output <span class="math">\(a_{1}\)</span> from the hidden layer. The idea behind backpropagation is to compute the error between ground truth and estimate, and then use that error to update the value of the weights and biases via gradient descent:
</p>
<div class="math">\begin{align*}
    w_{j + 1} &amp;= w_{j} - \eta \frac{\partial E}{\partial w_{j}} &amp; b_{j + 1} &amp;= b_{j} - \eta \frac{\partial E}{\partial b_{j}}
\end{align*}</div>
<p>
As an example, consider the following error function:
</p>
<div class="math">\begin{equation*}
    E = \frac{1}{2} \left( T - a_{2} \right)^{2}
\end{equation*}</div>
<p>
Note that <span class="math">\(E\)</span> depends implicitly on the two weights and the two biases. You need to update the two weights and the two biases. Consider the following gradients:
</p>
<div class="math">\begin{align*}
    \frac{\partial E}{\partial w_{1}} &amp;= -(T - a_{2}) \frac{\partial a_{2}}{\partial w_{1}} \\
    \frac{\partial E}{\partial w_{2}} &amp;= -(T - a_{2}) \frac{\partial a_{2}}{\partial w_{2}} \\
    \frac{\partial E}{\partial b_{1}} &amp;= -(T - a_{2}) \frac{\partial a_{2}}{\partial b_{1}} \\
    \frac{\partial E}{\partial b_{2}} &amp;= -(T - a_{2}) \frac{\partial a_{2}}{\partial b_{2}}
\end{align*}</div>
<p>
You need the gradients of the output <span class="math">\(a_{2}\)</span>:
</p>
<div class="math">\begin{align*}
    \frac{\partial a_{2}}{\partial w_{1}} &amp;= \frac{\partial f}{\partial z_{2}} \frac{\partial z_{2}}{\partial w_{1}} \\
    \frac{\partial a_{2}}{\partial w_{2}} &amp;= \frac{\partial f}{\partial z_{2}} \frac{\partial z_{2}}{\partial w_{2}} \\
    \frac{\partial a_{2}}{\partial b_{1}} &amp;= \frac{\partial f}{\partial z_{2}} \frac{\partial z_{2}}{\partial b_{1}} \\
    \frac{\partial a_{2}}{\partial b_{2}} &amp;= \frac{\partial f}{\partial z_{2}} \frac{\partial z_{2}}{\partial b_{2}}
\end{align*}</div>
<p>
Furthermore, you need the gradients of the activation function <span class="math">\(f\)</span>, and the result <span class="math">\(z_{2}\)</span> of the second hidden layer:
</p>
<div class="math">\begin{align*}
    \frac{\partial z_{2}}{\partial w_{1}} &amp;= w_{2} \frac{\partial a_{1}}{\partial w_{1}} \\
    \frac{\partial z_{2}}{\partial w_{2}} &amp;= a_{1} \\
    \frac{\partial z_{2}}{\partial b_{1}} &amp;= w_{2} \frac{\partial a_{1}}{\partial b_{1}} \\
    \frac{\partial z_{2}}{\partial b_{2}} &amp;= 1
\end{align*}</div>
<p>
Finally you need the gradients of the output <span class="math">\(a_{1}\)</span> of the first hidden layer:
</p>
<div class="math">\begin{align*}
    \frac{\partial a_{1}}{\partial w_{1}} &amp;= \frac{\partial f}{\partial z_{1}} \frac{\partial z_{1}}{\partial w_{1}} = \frac{\partial f}{\partial z_{1}} x_{1} \\
    \frac{\partial a_{1}}{\partial b_{1}} &amp;= \frac{\partial f}{\partial z_{1}} \frac{\partial z_{1}}{\partial b_{1}} = \frac{\partial f}{\partial z_{1}}
\end{align*}</div>
<p>
Thus,
</p>
<div class="math">\begin{align*}
    \frac{\partial E}{\partial w_{1}} &amp;= -(T - a_{2}) \frac{\partial f}{\partial z_{2}} \frac{\partial f}{\partial z_{1}} w_{2} x_{1} \\
    \frac{\partial E}{\partial w_{2}} &amp;= -(T - a_{2}) \frac{\partial f}{\partial z_{2}} a_{1} \\
    \frac{\partial E}{\partial b_{1}} &amp;= -(T - a_{2}) \frac{\partial f}{\partial z_{2}} \frac{\partial f}{\partial z_{1}} w_{2}  \\
    \frac{\partial E}{\partial b_{2}} &amp;= -(T - a_{2}) \frac{\partial f}{\partial z_{2}}
\end{align*}</div>
<p>
If <span class="math">\(f\)</span> is the <strong>logistic</strong> function,
</p>
<div class="math">\begin{equation*}
    f(z) = \frac{1}{1 + \exp(-z)}
\end{equation*}</div>
<p>
then
</p>
<div class="math">\begin{equation*}
    \frac{\partial f}{\partial z} = \frac{\exp(-z)}{\left[ 1 + \exp(-z) \right]^{2}} = f(z) \left[1 - f(z)\right]
\end{equation*}</div>
<p>
This means that
</p>
<div class="math">\begin{align*}
    \frac{\partial f}{\partial z_{1}} &amp;= a_{1} (1 - a_{1}) \\ \frac{\partial f}{\partial z_{2}} &amp;= a_{2} (1 - a_{2})
\end{align*}</div>
<p>
Gradient descent leads to the following updates for the weights and the biases:
</p>
<div class="math">\begin{align*}
    w_{1} &amp;\longrightarrow w_{1} + \eta (T - a_{2}) a_{2} (1 - a_{2}) a_{1} (1 - a_{1}) w_{2} x_{1} \\
    w_{2} &amp;\longrightarrow w_{2} + \eta (T - a_{2}) a_{2} (1 - a_{2}) a_{1} \\
    b_{1} &amp;\longrightarrow b_{1} + \eta (T - a_{2}) a_{2} (1 - a_{2}) a_{1} (1 - a_{1}) w_{2} \\
    b_{2} &amp;\longrightarrow b_{2} + \eta (T - a_{2}) a_{2} (1 - a_{2})
\end{align*}</div>
<p>
The outline of the training algorithm is as follows:</p>
<ol>
<li>Initialize weights and biases with random values.</li>
<li>Use forward propagation to calculate output of network.</li>
<li>Calculate error between truth and estimated output.</li>
<li>Check if the error is below a pre-defined threshold. If it is, stop. Otherwise, continue.</li>
<li>Update weights and biases via gradient descent and backpropagation.</li>
<li>If all iterations have been done, stop. Otherwise, go back to step 2.</li>
</ol>
<p>The logistic function allowed some convenience while doing the previous calculation by hand, but numerically it can lead to problems. One of them is the <strong>vanishing gradient problem</strong>, where small values for the gradients arise and get compounded by all the multiplications involve as you move more and more backward into the network. This leads to the layers closer to the output training differently than the layers farther away from the output.</p>
<p>In order to overcome the vanishing gradient problem, other activation functions besides the logistic function can be used. One such activation function is the Rectified Linear Unit or <strong>ReLU</strong>:
</p>
<div class="math">\begin{equation*}
    f(z) = \operatorname{max}(0, z)
\end{equation*}</div>
<p>
ReLU is usually used in hidden layers. Another example is the <strong>SoftMax</strong> function:
</p>
<div class="math">\begin{equation*}
    f(z_{j}) = \dfrac{\exp(z_{j})}{\displaystyle\sum_{k = 1}^{N} \exp(z_{k})}
\end{equation*}</div>
<p>
This one is usually used in the output layer of the network. Since
</p>
<div class="math">\begin{equation*}
    \sum_{j = 1}^{N} f(z_{j}) = 1
\end{equation*}</div>
<p>
the SoftMax output can be understood as a <strong>probability</strong>. Indeed, with
</p>
<div class="math">\begin{equation*}
    z_{j} = - \frac{E_{j}}{k_{B}T}
\end{equation*}</div>
<p>
you can recognize the partition function from thermal physics and statistical mechanics.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <ul class="col-sm-6 list-inline">
    <li class="list-inline-item"><a href="https://meirizarrygelpi.github.io/archives.html">Archives</a></li>
    <li class="list-inline-item"><a href="https://meirizarrygelpi.github.io/categories.html">Categories</a></li>
      <li class="list-inline-item"><a href="https://meirizarrygelpi.github.io/tags.html">Tags</a></li>
  </ul>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p>
</div>    </div>
  </footer>

</body>

</html>